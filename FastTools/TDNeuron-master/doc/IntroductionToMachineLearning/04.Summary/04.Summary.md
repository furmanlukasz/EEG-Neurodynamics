## Summary

Let us summarize so far what we've learned by using the right terminology.

The algorithm that we just implemented is called **Linear regression**. 

As you saw, linear regression consists on modelling the relationship between some variable and one or more parameters, using a linear function **``wx + b``** (a line).

**Linear regression** has two clear stages:

1. Forward propagation 
2. Backward propagation
    
**Forward propagation** consists of:

1. Creating a linear mapping using the ``wx + b`` equation.
2. Computing a **cost function** that tell us how well our model is performing over time (how close we are to the **ground truth**).
    
**Backward propagation** consists of:

1. Taking the partial derivatives of each forward step.
2.  Using the chain rule to compute the **gradient**, or direction on which our line needs to get tilted and moved, to get closer to the ground truth.
    
The process of updating our original weight and bias parameters is called **optimization**, because we are improving the model on every run.
    
A single run through the whole data set (input samples) is called an **epoch**.
    
    
Because we do this update by subtracting the gradient of weight and bias from the original weight and bias, we call this method **gradient descent**. We are truly "descending" from some state to others in tiny steps, to ultimately find our optimal state.
    
The tiny steps that we take are controlled by a factor called **learning rate**, which we can alter to learn slower or faster.

This complete process, where an algorithm optimizes itself to reach an optimal state based on a ground truth is referred to as **learning**. 

 The following chapters give an overview of various other elements that can be involved in the learning process, all of which are currently implemented in **TDNeuron**.

